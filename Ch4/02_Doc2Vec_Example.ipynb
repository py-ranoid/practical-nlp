{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCgVnQopb6TI"
   },
   "source": [
    "# Document Vectorization (Doc2Vec)\n",
    "\n",
    "In this notebook, let us take a look at how to \"learn\" document embeddings and use them for text classification. We will be using the dataset of \"Sentiment and Emotion in Text\" from [Kaggle](https://www.kaggle.com/c/sa-emotions/data).\n",
    "\n",
    "\"In a variation on the popular task of sentiment analysis, this dataset contains labels for the emotional content (such as happiness, sadness, and anger) of texts. Hundreds to thousands of examples across 13 labels. A subset of this data is used in an experiment we uploaded to Microsoftâ€™s Cortana Intelligence Gallery.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:05.748732Z",
     "start_time": "2020-09-06T06:07:04.504910Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "hSB6W1seb6TJ",
    "outputId": "fa6730c6-06df-46ce-91c8-cd59211d24de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vishalgupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, subprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:05.850098Z",
     "start_time": "2020-09-06T06:07:05.750951Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "Fnv168QRJjxi",
    "outputId": "a99d7d74-d248-4fa0-bf66-916283ffd401"
   },
   "outputs": [],
   "source": [
    "#downloding data\n",
    "DATA_PATH = \"Data\"\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\"\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_PATH, TRAIN_URL.split('/')[-1])\n",
    "if not os.path.exists(TRAIN_FILE_PATH):\n",
    "    process = subprocess.run(\"curl %s --output %s\"%(TRAIN_URL, TRAIN_FILE_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "\n",
    "TEST_URL = \"https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\"\n",
    "TEST_FILE_PATH = os.path.join(DATA_PATH, TEST_URL.split('/')[-1])\n",
    "if not os.path.exists(TEST_FILE_PATH):\n",
    "    process = subprocess.run(\"curl %s --output %s\"%(TEST_URL, TEST_FILE_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE_PATH)    \n",
    "test_df = pd.read_csv(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:05.899680Z",
     "start_time": "2020-09-06T06:07:05.852638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape: (30000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Training Data shape: %s\"%str(train_df.shape))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:05.938547Z",
     "start_time": "2020-09-06T06:07:05.902094Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "5JEI6SH7b6TU",
    "outputId": "22cc98a5-90d0-49c9-fb58-f40d743963e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         7433\n",
       "neutral       6340\n",
       "sadness       4828\n",
       "happiness     2986\n",
       "love          2068\n",
       "surprise      1613\n",
       "hate          1187\n",
       "fun           1088\n",
       "relief        1021\n",
       "empty          659\n",
       "enthusiasm     522\n",
       "boredom        157\n",
       "anger           98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the top 5 categories and leave out the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:05.979835Z",
     "start_time": "2020-09-06T06:07:05.940599Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CHajyKpmb6TY",
    "outputId": "8749211d-1a7c-43c9-bf40-d4d22e74407a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape after filtering top 3 sentiments : (23655, 2)\n"
     ]
    }
   ],
   "source": [
    "sentiment_shortlist = train_df['sentiment'].value_counts().nlargest(5).index\n",
    "train_df_subset = train_df[train_df['sentiment'].isin(sentiment_shortlist)]\n",
    "print(\"train_df shape after filtering top 3 sentiments : %s\"%str(train_df_subset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2oiZzU5b6Tf"
   },
   "source": [
    "## Pre-processing\n",
    "Tweets are different from raw text. Hence, they must also be pre-processed uniquely.\n",
    "- Remove @mentions, and URLs\n",
    "- Use NLTK Tweet tokenizer instead of a regular one.\n",
    "- Remove stopwords and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:06.016476Z",
     "start_time": "2020-09-06T06:07:05.981661Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rl-FfMdLb6Th",
    "outputId": "6273576c-0495-4606-da02-a7d06358ab2d"
   },
   "outputs": [],
   "source": [
    "#strip_handles removes personal information such as twitter handles, which don't\n",
    "#contribute to emotion in the tweet. preserve_case=False converts everything to lowercase.\n",
    "tweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Function to tokenize tweets, remove stopwords and numbers. \n",
    "#Keeping punctuations and emoticon symbols could be relevant for this task!\n",
    "def preprocess_corpus(texts):\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that removes stopwords and digits from a list of tokens\n",
    "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:08.740040Z",
     "start_time": "2020-09-06T06:07:06.018701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in X_train : 23655\n"
     ]
    }
   ],
   "source": [
    "X = preprocess_corpus(train_df_subset['content'])\n",
    "y = train_df_subset['sentiment']\n",
    "print(\"Number of rows in X_train : %d\"%len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:07:08.787141Z",
     "start_time": "2020-09-06T06:07:08.744668Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "rsGwfVebb6Tl",
    "outputId": "ce668f41-578a-467e-d3e7-20ea66b53c6d"
   },
   "outputs": [],
   "source": [
    "#Split data into train and test, following the usual process\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:10:01.298727Z",
     "start_time": "2020-09-06T06:07:08.790048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "#Prepare training data in doc2vec format:\n",
    "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(X_train)]\n",
    "\n",
    "#Train a doc2vec model to learn tweet representations. Use only training data!!\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n",
    "\n",
    "#Infer the feature representation for training and test data using the trained model\n",
    "# model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#Generate document vectors for \n",
    "X_train_vecs =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in X_train]\n",
    "X_test_vecs = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:10:01.730692Z",
     "start_time": "2020-09-06T06:10:01.300944Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "hTqo26Vsb6Ts",
    "outputId": "13f5218a-a22d-400d-bd9e-d53a51c767d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use any regular classifier like logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") #because classes are not balanced. \n",
    "logreg.fit(X_train_vecs, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T06:10:01.923023Z",
     "start_time": "2020-09-06T06:10:01.733544Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FQuoinmiK_jG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.28      0.27      0.28       768\n",
      "        love       0.21      0.43      0.28       501\n",
      "     neutral       0.38      0.46      0.42      1614\n",
      "     sadness       0.33      0.34      0.33      1183\n",
      "       worry       0.45      0.22      0.30      1848\n",
      "\n",
      "    accuracy                           0.34      5914\n",
      "   macro avg       0.33      0.35      0.32      5914\n",
      "weighted avg       0.36      0.34      0.33      5914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_test_preds = logreg.predict(X_test_vecs)\n",
    "print(classification_report(y_test, y_test_preds))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Doc2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
