{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVtvH58nb_Hp"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
    "\n",
    "We will use the sentiment labelled sentences dataset from UCI repository\n",
    "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
    "\n",
    "Let us get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:57:50.260070Z",
     "start_time": "2021-01-03T14:57:49.038259Z"
    }
   },
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import os, subprocess\n",
    "from time import time\n",
    "\n",
    "#pre-processing imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data & Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:57:50.302528Z",
     "start_time": "2021-01-03T14:57:50.263129Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#Download Training data\n",
    "DATA_PATH = \"Data\"\n",
    "TRAIN_ZIP_URL = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
    "TRAIN_ZIP_PATH = os.path.join(DATA_PATH, TRAIN_ZIP_URL.split('/')[-1].replace(\"%20\",\" \"))\n",
    "TRAIN_FOLDER_PATH = TRAIN_ZIP_PATH.replace('.zip','')\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_FOLDER_PATH, 'sentiment_sentences.txt')\n",
    "\n",
    "if not os.path.exists(TRAIN_ZIP_PATH):\n",
    "    process = subprocess.run('curl \"%s\" --output \"%s\"'%(TRAIN_ZIP_URL, TRAIN_ZIP_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "\n",
    "if not os.path.exists(TRAIN_FOLDER_PATH):\n",
    "    with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "if not os.path.exists(TRAIN_DATA_PATH):\n",
    "    subprocess.run('cd \"%s\" && cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt'%TRAIN_FOLDER_PATH, shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:57:50.337817Z",
     "start_time": "2021-01-03T14:57:50.304480Z"
    }
   },
   "outputs": [],
   "source": [
    "#Download Word2Vec model\n",
    "WORD2VEC_URL = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "WORD2VEC_PATH = os.path.join(DATA_PATH, WORD2VEC_URL.split('/')[-1])\n",
    "if not os.path.exists(WORD2VEC_PATH):\n",
    "    process = subprocess.run('curl \"%s\" --output \"%s\"'%(WORD2VEC_URL, WORD2VEC_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:43.710074Z",
     "start_time": "2021-01-03T14:57:50.339834Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "COUGXAxcb_H5",
    "outputId": "f1b6d8ad-e22b-4126-d2ea-862697c4158b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load W2V model. This will take some time. \n",
    "w2v_model = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)\n",
    "\n",
    "#Read text data and labels.\n",
    "data_df = pd.read_csv(TRAIN_DATA_PATH,sep='\\t',header=None).rename(columns={0:'text',1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:43.779958Z",
     "start_time": "2021-01-03T14:58:43.714034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:43.826704Z",
     "start_time": "2021-01-03T14:58:43.781901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1386\n",
       "0    1362\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:44.496504Z",
     "start_time": "2021-01-03T14:58:43.830663Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "m-WjFyC6b_IE",
    "outputId": "5df9e11b-6f8e-42b8-e198-6fe343293cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect the word2vec model\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Convert tokens to lowercase\n",
    "- Remove stopwords\n",
    "- Remove numbers and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.031237Z",
     "start_time": "2021-01-03T14:58:44.498855Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MFOGaDTwb_Ig",
    "outputId": "7603e297-9167-43ec-c7da-46d82dc850ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, way, plug, us, unless, i, go, converter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, case, excellent, value]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, jawbone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tied, charger, conversations, lasting, minute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, mic, great]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  So there is no way for me to plug it in here i...      0   \n",
       "1                        Good case, Excellent value.      1   \n",
       "2                             Great for the jawbone.      1   \n",
       "3  Tied to charger for conversations lasting more...      0   \n",
       "4                                  The mic is great.      1   \n",
       "\n",
       "                                              tokens  \n",
       "0      [so, way, plug, us, unless, i, go, converter]  \n",
       "1                     [good, case, excellent, value]  \n",
       "2                                   [great, jawbone]  \n",
       "3  [tied, charger, conversations, lasting, minute...  \n",
       "4                                  [the, mic, great]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the text.\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "def preprocess_corpus(text):\n",
    "    #Nested function that converts token to lowercase and removes stopwords & digits from a list of tokens        \n",
    "    tokens = word_tokenize(text)\n",
    "    keep_token = lambda token: token not in mystopwords and not token.isdigit() and token not in punctuation\n",
    "    return [token.lower() for token in tokens if keep_token(token)]\n",
    "\n",
    "data_df['tokens'] = data_df['text'].apply(preprocess_corpus)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Documents\n",
    "- Get mean of all token vectors in a document to get document vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.227096Z",
     "start_time": "2021-01-03T14:58:45.032974Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fXRiGtY1b_Iq",
    "outputId": "2d57a96f-8da8-4285-ca1e-2c617578b9e1"
   },
   "outputs": [],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "WORD2VEC_DIMENSION = 300\n",
    "def get_doc_vec(tokens):\n",
    "    token_vecs = np.array([w2v_model[token] for token in tokens if token in w2v_model])\n",
    "    doc_vec = token_vecs.mean(axis=0) if len(token_vecs)>0 else np.zeros(WORD2VEC_DIMENSION)\n",
    "    return doc_vec\n",
    "\n",
    "data_df['text_vec'] = data_df['tokens'].apply(get_doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.375305Z",
     "start_time": "2021-01-03T14:58:45.229445Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mr9IaQppb_Ix",
    "outputId": "13a84b5c-fde3-49f4-b156-5c2f36592b19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1234)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(data_df['text_vec'].apply(lambda x:x.tolist()), data_df['label'])\n",
    "classifier.fit(train_data.to_list(), train_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.443555Z",
     "start_time": "2021-01-03T14:58:45.377440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8384279475982532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84       357\n",
      "           1       0.83      0.83      0.83       330\n",
      "\n",
      "    accuracy                           0.84       687\n",
      "   macro avg       0.84      0.84      0.84       687\n",
      "weighted avg       0.84      0.84      0.84       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lambda text : classifier.predict_proba([get_doc_vec(preprocess_corpus(text))])[0,1]\n",
    "# pred = lambda text : classifier.predict([get_doc_vec(preprocess_corpus(text))])\n",
    "print(\"Accuracy: \", classifier.score(test_data.to_list(), test_cats))\n",
    "preds = classifier.predict(test_data.to_list())\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7wjLB8rb_JB"
   },
   "source": [
    "With little efforts we got 84% accuracy. Thats a great starting model to have!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.483032Z",
     "start_time": "2021-01-03T14:58:45.445938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9445554715124546"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred('Enjoyed the show. Will try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:58:45.523680Z",
     "start_time": "2021-01-03T14:58:45.485468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017510823645095952"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred('Service was unsatisfactory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing postive and negative tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T14:59:54.308791Z",
     "start_time": "2021-01-03T14:59:52.497701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tokens (top 20 from corpus): \n",
      "wonderful, beautifully, delightful, sweetest, joy, beautiful, versatile, fabulous, elegant, heartwarming, terrific, exciting, superb, wonderfully, lovely, timeless, enjoyed, stylish, fantastic, enjoy\n",
      "\n",
      "Negative tokens sample (top 20 from corpus): \n",
      "abysmal, unusable, insult, disgrace, inexcusable, vomit, misleading, unsatisfactory, unprofessional, embarassing, uninspired, stinker, blame, subpar, vomited, insipid, unreliable, disgraceful, undercooked, disgusted\n"
     ]
    }
   ],
   "source": [
    "all_tokens = pd.Series(list(set(sum(data_df['tokens'].to_list(),[]))))\n",
    "all_token_scores = all_tokens.apply(pred)\n",
    "positive_tokens = all_token_scores.set_axis(all_tokens).nlargest(20).index.to_list()\n",
    "negative_tokens = all_token_scores.set_axis(all_tokens).nsmallest(20).index.to_list()\n",
    "print (\"Positive tokens (top 20 from corpus): \\n%s\"%', '.join(positive_tokens))\n",
    "print (\"\\nNegative tokens sample (top 20 from corpus): \\n%s\"%', '.join(negative_tokens))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
