{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVtvH58nb_Hp"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
    "\n",
    "We will use the sentiment labelled sentences dataset from UCI repository\n",
    "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
    "\n",
    "Let us get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:43:18.967893Z",
     "start_time": "2021-01-03T09:43:17.706758Z"
    }
   },
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import os, subprocess\n",
    "from time import time\n",
    "\n",
    "#pre-processing imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data & Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:43:23.894264Z",
     "start_time": "2021-01-03T09:43:23.857356Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#Download Training data\n",
    "DATA_PATH = \"Data\"\n",
    "TRAIN_ZIP_URL = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
    "TRAIN_ZIP_PATH = os.path.join(DATA_PATH, TRAIN_ZIP_URL.split('/')[-1].replace(\"%20\",\" \"))\n",
    "TRAIN_FOLDER_PATH = TRAIN_ZIP_PATH.replace('.zip','')\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_FOLDER_PATH, 'sentiment_sentences.txt')\n",
    "\n",
    "if not os.path.exists(TRAIN_ZIP_PATH):\n",
    "    process = subprocess.run('curl \"%s\" --output \"%s\"'%(TRAIN_ZIP_URL, TRAIN_ZIP_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "\n",
    "if not os.path.exists(TRAIN_FOLDER_PATH):\n",
    "    with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "if not os.path.exists(TRAIN_DATA_PATH):\n",
    "    subprocess.run('cd \"%s\" && cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt'%TRAIN_FOLDER_PATH, shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:43:25.617875Z",
     "start_time": "2021-01-03T09:43:25.584927Z"
    }
   },
   "outputs": [],
   "source": [
    "#Download Word2Vec model\n",
    "WORD2VEC_URL = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "WORD2VEC_PATH = os.path.join(DATA_PATH, WORD2VEC_URL.split('/')[-1])\n",
    "if not os.path.exists(WORD2VEC_PATH):\n",
    "    process = subprocess.run('curl \"%s\" --output \"%s\"'%(WORD2VEC_URL, WORD2VEC_PATH), shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:44:23.804818Z",
     "start_time": "2021-01-03T09:43:29.146284Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "COUGXAxcb_H5",
    "outputId": "f1b6d8ad-e22b-4126-d2ea-862697c4158b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "#Load W2V model. This will take some time. \n",
    "w2v_model = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)\n",
    "print('done loading Word2Vec')\n",
    "\n",
    "#Read text data, cats.\n",
    "#the file path consists of tab separated sentences and cats.\n",
    "texts = []\n",
    "cats = []\n",
    "fh = open(TRAIN_DATA_PATH)\n",
    "for line in fh:\n",
    "    text, sentiment = line.split(\"\\t\")\n",
    "    texts.append(text.strip())\n",
    "    cats.append(sentiment.strip())\n",
    "data_df = pd.DataFrame({\"text\":texts,'label':cats})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:43.775729Z",
     "start_time": "2021-01-03T09:45:43.718037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  So there is no way for me to plug it in here i...     0\n",
       "1                        Good case, Excellent value.     1\n",
       "2                             Great for the jawbone.     1\n",
       "3  Tied to charger for conversations lasting more...     0\n",
       "4                                  The mic is great.     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:46.160973Z",
     "start_time": "2021-01-03T09:45:46.124637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1500\n",
       "0    1500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:47.842212Z",
     "start_time": "2021-01-03T09:45:47.180383Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "m-WjFyC6b_IE",
    "outputId": "5df9e11b-6f8e-42b8-e198-6fe343293cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:50.190264Z",
     "start_time": "2021-01-03T09:45:49.677779Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MFOGaDTwb_Ig",
    "outputId": "7603e297-9167-43ec-c7da-46d82dc850ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, way, plug, us, unless, i, go, converter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, case, excellent, value]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, jawbone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tied, charger, conversations, lasting, minute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, mic, great]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  So there is no way for me to plug it in here i...     0   \n",
       "1                        Good case, Excellent value.     1   \n",
       "2                             Great for the jawbone.     1   \n",
       "3  Tied to charger for conversations lasting more...     0   \n",
       "4                                  The mic is great.     1   \n",
       "\n",
       "                                              tokens  \n",
       "0      [so, way, plug, us, unless, i, go, converter]  \n",
       "1                     [good, case, excellent, value]  \n",
       "2                                   [great, jawbone]  \n",
       "3  [tied, charger, conversations, lasting, minute...  \n",
       "4                                  [the, mic, great]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the text.\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "def preprocess_corpus(text):\n",
    "    #Nested function that converts token to lowercase and removes stopwords & digits from a list of tokens        \n",
    "    tokens = word_tokenize(text)\n",
    "    keep_token = lambda token: token not in mystopwords and not token.isdigit() and token not in punctuation\n",
    "    return [token.lower() for token in tokens if keep_token(token)]\n",
    "\n",
    "data_df['tokens'] = data_df['text'].apply(preprocess_corpus)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Documents\n",
    "- Get mean of all token vectors in a document to get document vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:53.900013Z",
     "start_time": "2021-01-03T09:45:53.715887Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fXRiGtY1b_Iq",
    "outputId": "2d57a96f-8da8-4285-ca1e-2c617578b9e1"
   },
   "outputs": [],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "WORD2VEC_DIMENSION = 300\n",
    "def get_doc_vec(tokens):\n",
    "    token_vecs = np.array([w2v_model[token] for token in tokens if token in w2v_model])\n",
    "    doc_vec = token_vecs.mean(axis=0) if len(token_vecs)>0 else np.zeros(WORD2VEC_DIMENSION)\n",
    "    return doc_vec\n",
    "\n",
    "data_df['text_vec'] = data_df['tokens'].apply(get_doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:55.197277Z",
     "start_time": "2021-01-03T09:45:55.061912Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mr9IaQppb_Ix",
    "outputId": "13a84b5c-fde3-49f4-b156-5c2f36592b19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1234)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(data_df['text_vec'].apply(lambda x:x.tolist()), data_df['label'])\n",
    "classifier.fit(train_data.to_list(), train_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:45:56.685797Z",
     "start_time": "2021-01-03T09:45:56.569340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8573333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       371\n",
      "           1       0.88      0.83      0.86       379\n",
      "\n",
      "    accuracy                           0.86       750\n",
      "   macro avg       0.86      0.86      0.86       750\n",
      "weighted avg       0.86      0.86      0.86       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lambda text : classifier.predict([get_doc_vec(preprocess_corpus(text))])\n",
    "print(\"Accuracy: \", classifier.score(test_data.to_list(), test_cats))\n",
    "preds = classifier.predict(test_data.to_list())\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7wjLB8rb_JB"
   },
   "source": [
    "With little efforts we got 86% accuracy. Thats a great starting model to have!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:46:29.468989Z",
     "start_time": "2021-01-03T09:46:29.435528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred('Enjoyed the show. Will try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T09:46:30.916627Z",
     "start_time": "2021-01-03T09:46:30.883270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred('Service was unsatisfactory!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
